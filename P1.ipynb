{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBf6NVDwrZLX"
      },
      "source": [
        "#Program 1\n",
        "***Design and implement a neural based network for generating word embedding for words in a document corpus.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmhfuI269f09"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U76FLHMl9jF-"
      },
      "outputs": [],
      "source": [
        "corpus=[\n",
        "    \"Deep learning is a core subject of artificial intelligence\",\n",
        "    \"Machine learning is a subbranch of deep learning\",\n",
        "    \"Convolutional Neural Network (CNN) is a basic deep neural network in deep learning\",\n",
        "    \"Alex and Visual Geometry Group (VGG) neural networks are pre trained deep neural networks\",\n",
        "    \"Deep residual network is used in image recognition\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gh5eaiZn9n3l"
      },
      "outputs": [],
      "source": [
        "tokenized_corpus=[simple_preprocess(line) for line in corpus]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8LUzKbV9qyJ",
        "outputId": "ef740cb8-09bc-47a8-b2d5-1af99c12138c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['deep', 'learning', 'is', 'core', 'subject', 'of', 'artificial', 'intelligence'], ['machine', 'learning', 'is', 'subbranch', 'of', 'deep', 'learning'], ['convolutional', 'neural', 'network', 'cnn', 'is', 'basic', 'deep', 'neural', 'network', 'in', 'deep', 'learning'], ['alex', 'and', 'visual', 'geometry', 'group', 'vgg', 'neural', 'networks', 'are', 'pre', 'trained', 'deep', 'neural', 'networks'], ['deep', 'residual', 'network', 'is', 'used', 'in', 'image', 'recognition']]\n"
          ]
        }
      ],
      "source": [
        "print(tokenized_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JW_ubHKS9spi"
      },
      "outputs": [],
      "source": [
        "model = Word2Vec(\n",
        "    sentences=tokenized_corpus,\n",
        "    vector_size=300,\n",
        "    window=3,\n",
        "    min_count=1,\n",
        "    sg=1,\n",
        "    epochs=100\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiRAJfLi9vF_"
      },
      "outputs": [],
      "source": [
        "words = [word for sentence in tokenized_corpus for word in sentence]\n",
        "vocab = set(words)\n",
        "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx2word = {idx: word for word, idx in word2idx.items()}\n",
        "vocab_size = len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePdXhOKd9xfW",
        "outputId": "c6414586-ef2c-47c9-d248-cfc3a73c6e09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['deep', 'learning', 'is', 'core', 'subject', 'of', 'artificial', 'intelligence', 'machine', 'learning', 'is', 'subbranch', 'of', 'deep', 'learning', 'convolutional', 'neural', 'network', 'cnn', 'is', 'basic', 'deep', 'neural', 'network', 'in', 'deep', 'learning', 'alex', 'and', 'visual', 'geometry', 'group', 'vgg', 'neural', 'networks', 'are', 'pre', 'trained', 'deep', 'neural', 'networks', 'deep', 'residual', 'network', 'is', 'used', 'in', 'image', 'recognition']\n"
          ]
        }
      ],
      "source": [
        "print(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyBaC3pi9z7R",
        "outputId": "148541bc-c095-491e-e8ef-30d12094d5ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'cnn', 'group', 'used', 'neural', 'learning', 'core', 'subject', 'machine', 'recognition', 'networks', 'pre', 'trained', 'network', 'image', 'intelligence', 'geometry', 'residual', 'subbranch', 'are', 'basic', 'vgg', 'in', 'and', 'of', 'alex', 'is', 'deep', 'artificial', 'convolutional', 'visual'}\n"
          ]
        }
      ],
      "source": [
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBO5fN8T92F_",
        "outputId": "3cca1787-172d-45dd-cd1b-fba43e3483ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30\n"
          ]
        }
      ],
      "source": [
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BAj6F2Y95Uj",
        "outputId": "397ede92-7277-41f4-9bf8-aad47693c3e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/100], Loss: 0.0001\n",
            "Epoch [10/100], Loss: 0.0004\n",
            "Epoch [10/100], Loss: 0.0006\n",
            "Epoch [10/100], Loss: 0.0017\n",
            "Epoch [10/100], Loss: 0.0027\n",
            "Epoch [10/100], Loss: 0.0031\n",
            "Epoch [10/100], Loss: 0.0043\n",
            "Epoch [10/100], Loss: 0.0050\n",
            "Epoch [10/100], Loss: 0.0060\n",
            "Epoch [10/100], Loss: 0.0063\n",
            "Epoch [10/100], Loss: 0.0065\n",
            "Epoch [10/100], Loss: 0.0074\n",
            "Epoch [10/100], Loss: 0.0078\n",
            "Epoch [10/100], Loss: 0.0079\n",
            "Epoch [10/100], Loss: 0.0081\n",
            "Epoch [10/100], Loss: 0.0092\n",
            "Epoch [10/100], Loss: 0.0095\n",
            "Epoch [10/100], Loss: 0.0098\n",
            "Epoch [10/100], Loss: 0.0107\n",
            "Epoch [10/100], Loss: 0.0110\n",
            "Epoch [10/100], Loss: 0.0118\n",
            "Epoch [10/100], Loss: 0.0119\n",
            "Epoch [10/100], Loss: 0.0122\n",
            "Epoch [10/100], Loss: 0.0125\n",
            "Epoch [10/100], Loss: 0.0131\n",
            "Epoch [10/100], Loss: 0.0132\n",
            "Epoch [10/100], Loss: 0.0134\n",
            "Epoch [10/100], Loss: 0.0141\n",
            "Epoch [10/100], Loss: 0.0151\n",
            "Epoch [10/100], Loss: 0.0158\n",
            "Epoch [10/100], Loss: 0.0167\n",
            "Epoch [10/100], Loss: 0.0175\n",
            "Epoch [10/100], Loss: 0.0182\n",
            "Epoch [10/100], Loss: 0.0185\n",
            "Epoch [10/100], Loss: 0.0190\n",
            "Epoch [10/100], Loss: 0.0199\n",
            "Epoch [10/100], Loss: 0.0207\n",
            "Epoch [10/100], Loss: 0.0214\n",
            "Epoch [10/100], Loss: 0.0215\n",
            "Epoch [10/100], Loss: 0.0217\n",
            "Epoch [10/100], Loss: 0.0222\n",
            "Epoch [10/100], Loss: 0.0224\n",
            "Epoch [10/100], Loss: 0.0232\n",
            "Epoch [10/100], Loss: 0.0235\n",
            "Epoch [10/100], Loss: 0.0237\n",
            "Epoch [10/100], Loss: 0.0244\n",
            "Epoch [10/100], Loss: 0.0249\n",
            "Epoch [10/100], Loss: 0.0258\n",
            "Epoch [10/100], Loss: 0.0266\n",
            "Epoch [20/100], Loss: 0.0000\n",
            "Epoch [20/100], Loss: 0.0001\n",
            "Epoch [20/100], Loss: 0.0002\n",
            "Epoch [20/100], Loss: 0.0005\n",
            "Epoch [20/100], Loss: 0.0008\n",
            "Epoch [20/100], Loss: 0.0009\n",
            "Epoch [20/100], Loss: 0.0012\n",
            "Epoch [20/100], Loss: 0.0014\n",
            "Epoch [20/100], Loss: 0.0017\n",
            "Epoch [20/100], Loss: 0.0018\n",
            "Epoch [20/100], Loss: 0.0018\n",
            "Epoch [20/100], Loss: 0.0021\n",
            "Epoch [20/100], Loss: 0.0022\n",
            "Epoch [20/100], Loss: 0.0022\n",
            "Epoch [20/100], Loss: 0.0023\n",
            "Epoch [20/100], Loss: 0.0026\n",
            "Epoch [20/100], Loss: 0.0027\n",
            "Epoch [20/100], Loss: 0.0028\n",
            "Epoch [20/100], Loss: 0.0030\n",
            "Epoch [20/100], Loss: 0.0031\n",
            "Epoch [20/100], Loss: 0.0034\n",
            "Epoch [20/100], Loss: 0.0034\n",
            "Epoch [20/100], Loss: 0.0035\n",
            "Epoch [20/100], Loss: 0.0036\n",
            "Epoch [20/100], Loss: 0.0037\n",
            "Epoch [20/100], Loss: 0.0038\n",
            "Epoch [20/100], Loss: 0.0038\n",
            "Epoch [20/100], Loss: 0.0041\n",
            "Epoch [20/100], Loss: 0.0043\n",
            "Epoch [20/100], Loss: 0.0045\n",
            "Epoch [20/100], Loss: 0.0048\n",
            "Epoch [20/100], Loss: 0.0050\n",
            "Epoch [20/100], Loss: 0.0052\n",
            "Epoch [20/100], Loss: 0.0053\n",
            "Epoch [20/100], Loss: 0.0055\n",
            "Epoch [20/100], Loss: 0.0057\n",
            "Epoch [20/100], Loss: 0.0060\n",
            "Epoch [20/100], Loss: 0.0062\n",
            "Epoch [20/100], Loss: 0.0062\n",
            "Epoch [20/100], Loss: 0.0063\n",
            "Epoch [20/100], Loss: 0.0065\n",
            "Epoch [20/100], Loss: 0.0065\n",
            "Epoch [20/100], Loss: 0.0067\n",
            "Epoch [20/100], Loss: 0.0068\n",
            "Epoch [20/100], Loss: 0.0069\n",
            "Epoch [20/100], Loss: 0.0071\n",
            "Epoch [20/100], Loss: 0.0073\n",
            "Epoch [20/100], Loss: 0.0075\n",
            "Epoch [20/100], Loss: 0.0078\n",
            "Epoch [30/100], Loss: 0.0000\n",
            "Epoch [30/100], Loss: 0.0001\n",
            "Epoch [30/100], Loss: 0.0001\n",
            "Epoch [30/100], Loss: 0.0002\n",
            "Epoch [30/100], Loss: 0.0004\n",
            "Epoch [30/100], Loss: 0.0004\n",
            "Epoch [30/100], Loss: 0.0006\n",
            "Epoch [30/100], Loss: 0.0007\n",
            "Epoch [30/100], Loss: 0.0008\n",
            "Epoch [30/100], Loss: 0.0008\n",
            "Epoch [30/100], Loss: 0.0009\n",
            "Epoch [30/100], Loss: 0.0010\n",
            "Epoch [30/100], Loss: 0.0011\n",
            "Epoch [30/100], Loss: 0.0011\n",
            "Epoch [30/100], Loss: 0.0011\n",
            "Epoch [30/100], Loss: 0.0012\n",
            "Epoch [30/100], Loss: 0.0013\n",
            "Epoch [30/100], Loss: 0.0013\n",
            "Epoch [30/100], Loss: 0.0015\n",
            "Epoch [30/100], Loss: 0.0015\n",
            "Epoch [30/100], Loss: 0.0016\n",
            "Epoch [30/100], Loss: 0.0016\n",
            "Epoch [30/100], Loss: 0.0017\n",
            "Epoch [30/100], Loss: 0.0017\n",
            "Epoch [30/100], Loss: 0.0018\n",
            "Epoch [30/100], Loss: 0.0018\n",
            "Epoch [30/100], Loss: 0.0018\n",
            "Epoch [30/100], Loss: 0.0019\n",
            "Epoch [30/100], Loss: 0.0021\n",
            "Epoch [30/100], Loss: 0.0022\n",
            "Epoch [30/100], Loss: 0.0023\n",
            "Epoch [30/100], Loss: 0.0024\n",
            "Epoch [30/100], Loss: 0.0025\n",
            "Epoch [30/100], Loss: 0.0026\n",
            "Epoch [30/100], Loss: 0.0026\n",
            "Epoch [30/100], Loss: 0.0028\n",
            "Epoch [30/100], Loss: 0.0029\n",
            "Epoch [30/100], Loss: 0.0030\n",
            "Epoch [30/100], Loss: 0.0030\n",
            "Epoch [30/100], Loss: 0.0030\n",
            "Epoch [30/100], Loss: 0.0031\n",
            "Epoch [30/100], Loss: 0.0031\n",
            "Epoch [30/100], Loss: 0.0032\n",
            "Epoch [30/100], Loss: 0.0033\n",
            "Epoch [30/100], Loss: 0.0033\n",
            "Epoch [30/100], Loss: 0.0034\n",
            "Epoch [30/100], Loss: 0.0035\n",
            "Epoch [30/100], Loss: 0.0036\n",
            "Epoch [30/100], Loss: 0.0037\n",
            "Epoch [40/100], Loss: 0.0000\n",
            "Epoch [40/100], Loss: 0.0000\n",
            "Epoch [40/100], Loss: 0.0000\n",
            "Epoch [40/100], Loss: 0.0001\n",
            "Epoch [40/100], Loss: 0.0002\n",
            "Epoch [40/100], Loss: 0.0002\n",
            "Epoch [40/100], Loss: 0.0003\n",
            "Epoch [40/100], Loss: 0.0004\n",
            "Epoch [40/100], Loss: 0.0005\n",
            "Epoch [40/100], Loss: 0.0005\n",
            "Epoch [40/100], Loss: 0.0005\n",
            "Epoch [40/100], Loss: 0.0006\n",
            "Epoch [40/100], Loss: 0.0006\n",
            "Epoch [40/100], Loss: 0.0006\n",
            "Epoch [40/100], Loss: 0.0006\n",
            "Epoch [40/100], Loss: 0.0007\n",
            "Epoch [40/100], Loss: 0.0007\n",
            "Epoch [40/100], Loss: 0.0008\n",
            "Epoch [40/100], Loss: 0.0008\n",
            "Epoch [40/100], Loss: 0.0009\n",
            "Epoch [40/100], Loss: 0.0009\n",
            "Epoch [40/100], Loss: 0.0009\n",
            "Epoch [40/100], Loss: 0.0010\n",
            "Epoch [40/100], Loss: 0.0010\n",
            "Epoch [40/100], Loss: 0.0010\n",
            "Epoch [40/100], Loss: 0.0010\n",
            "Epoch [40/100], Loss: 0.0011\n",
            "Epoch [40/100], Loss: 0.0011\n",
            "Epoch [40/100], Loss: 0.0012\n",
            "Epoch [40/100], Loss: 0.0013\n",
            "Epoch [40/100], Loss: 0.0013\n",
            "Epoch [40/100], Loss: 0.0014\n",
            "Epoch [40/100], Loss: 0.0015\n",
            "Epoch [40/100], Loss: 0.0015\n",
            "Epoch [40/100], Loss: 0.0015\n",
            "Epoch [40/100], Loss: 0.0016\n",
            "Epoch [40/100], Loss: 0.0017\n",
            "Epoch [40/100], Loss: 0.0017\n",
            "Epoch [40/100], Loss: 0.0017\n",
            "Epoch [40/100], Loss: 0.0018\n",
            "Epoch [40/100], Loss: 0.0018\n",
            "Epoch [40/100], Loss: 0.0018\n",
            "Epoch [40/100], Loss: 0.0019\n",
            "Epoch [40/100], Loss: 0.0019\n",
            "Epoch [40/100], Loss: 0.0019\n",
            "Epoch [40/100], Loss: 0.0020\n",
            "Epoch [40/100], Loss: 0.0020\n",
            "Epoch [40/100], Loss: 0.0021\n",
            "Epoch [40/100], Loss: 0.0022\n",
            "Epoch [50/100], Loss: 0.0000\n",
            "Epoch [50/100], Loss: 0.0000\n",
            "Epoch [50/100], Loss: 0.0000\n",
            "Epoch [50/100], Loss: 0.0001\n",
            "Epoch [50/100], Loss: 0.0001\n",
            "Epoch [50/100], Loss: 0.0002\n",
            "Epoch [50/100], Loss: 0.0002\n",
            "Epoch [50/100], Loss: 0.0002\n",
            "Epoch [50/100], Loss: 0.0003\n",
            "Epoch [50/100], Loss: 0.0003\n",
            "Epoch [50/100], Loss: 0.0003\n",
            "Epoch [50/100], Loss: 0.0004\n",
            "Epoch [50/100], Loss: 0.0004\n",
            "Epoch [50/100], Loss: 0.0004\n",
            "Epoch [50/100], Loss: 0.0004\n",
            "Epoch [50/100], Loss: 0.0005\n",
            "Epoch [50/100], Loss: 0.0005\n",
            "Epoch [50/100], Loss: 0.0005\n",
            "Epoch [50/100], Loss: 0.0005\n",
            "Epoch [50/100], Loss: 0.0005\n",
            "Epoch [50/100], Loss: 0.0006\n",
            "Epoch [50/100], Loss: 0.0006\n",
            "Epoch [50/100], Loss: 0.0006\n",
            "Epoch [50/100], Loss: 0.0006\n",
            "Epoch [50/100], Loss: 0.0007\n",
            "Epoch [50/100], Loss: 0.0007\n",
            "Epoch [50/100], Loss: 0.0007\n",
            "Epoch [50/100], Loss: 0.0007\n",
            "Epoch [50/100], Loss: 0.0008\n",
            "Epoch [50/100], Loss: 0.0008\n",
            "Epoch [50/100], Loss: 0.0009\n",
            "Epoch [50/100], Loss: 0.0009\n",
            "Epoch [50/100], Loss: 0.0009\n",
            "Epoch [50/100], Loss: 0.0009\n",
            "Epoch [50/100], Loss: 0.0010\n",
            "Epoch [50/100], Loss: 0.0010\n",
            "Epoch [50/100], Loss: 0.0011\n",
            "Epoch [50/100], Loss: 0.0011\n",
            "Epoch [50/100], Loss: 0.0011\n",
            "Epoch [50/100], Loss: 0.0011\n",
            "Epoch [50/100], Loss: 0.0012\n",
            "Epoch [50/100], Loss: 0.0012\n",
            "Epoch [50/100], Loss: 0.0012\n",
            "Epoch [50/100], Loss: 0.0012\n",
            "Epoch [50/100], Loss: 0.0012\n",
            "Epoch [50/100], Loss: 0.0013\n",
            "Epoch [50/100], Loss: 0.0013\n",
            "Epoch [50/100], Loss: 0.0013\n",
            "Epoch [50/100], Loss: 0.0014\n",
            "Epoch [60/100], Loss: 0.0000\n",
            "Epoch [60/100], Loss: 0.0000\n",
            "Epoch [60/100], Loss: 0.0000\n",
            "Epoch [60/100], Loss: 0.0001\n",
            "Epoch [60/100], Loss: 0.0001\n",
            "Epoch [60/100], Loss: 0.0001\n",
            "Epoch [60/100], Loss: 0.0001\n",
            "Epoch [60/100], Loss: 0.0002\n",
            "Epoch [60/100], Loss: 0.0002\n",
            "Epoch [60/100], Loss: 0.0002\n",
            "Epoch [60/100], Loss: 0.0002\n",
            "Epoch [60/100], Loss: 0.0002\n",
            "Epoch [60/100], Loss: 0.0003\n",
            "Epoch [60/100], Loss: 0.0003\n",
            "Epoch [60/100], Loss: 0.0003\n",
            "Epoch [60/100], Loss: 0.0003\n",
            "Epoch [60/100], Loss: 0.0003\n",
            "Epoch [60/100], Loss: 0.0003\n",
            "Epoch [60/100], Loss: 0.0004\n",
            "Epoch [60/100], Loss: 0.0004\n",
            "Epoch [60/100], Loss: 0.0004\n",
            "Epoch [60/100], Loss: 0.0004\n",
            "Epoch [60/100], Loss: 0.0004\n",
            "Epoch [60/100], Loss: 0.0004\n",
            "Epoch [60/100], Loss: 0.0004\n",
            "Epoch [60/100], Loss: 0.0005\n",
            "Epoch [60/100], Loss: 0.0005\n",
            "Epoch [60/100], Loss: 0.0005\n",
            "Epoch [60/100], Loss: 0.0005\n",
            "Epoch [60/100], Loss: 0.0005\n",
            "Epoch [60/100], Loss: 0.0006\n",
            "Epoch [60/100], Loss: 0.0006\n",
            "Epoch [60/100], Loss: 0.0006\n",
            "Epoch [60/100], Loss: 0.0006\n",
            "Epoch [60/100], Loss: 0.0007\n",
            "Epoch [60/100], Loss: 0.0007\n",
            "Epoch [60/100], Loss: 0.0007\n",
            "Epoch [60/100], Loss: 0.0007\n",
            "Epoch [60/100], Loss: 0.0008\n",
            "Epoch [60/100], Loss: 0.0008\n",
            "Epoch [60/100], Loss: 0.0008\n",
            "Epoch [60/100], Loss: 0.0008\n",
            "Epoch [60/100], Loss: 0.0008\n",
            "Epoch [60/100], Loss: 0.0008\n",
            "Epoch [60/100], Loss: 0.0008\n",
            "Epoch [60/100], Loss: 0.0009\n",
            "Epoch [60/100], Loss: 0.0009\n",
            "Epoch [60/100], Loss: 0.0009\n",
            "Epoch [60/100], Loss: 0.0009\n",
            "Epoch [70/100], Loss: 0.0000\n",
            "Epoch [70/100], Loss: 0.0000\n",
            "Epoch [70/100], Loss: 0.0000\n",
            "Epoch [70/100], Loss: 0.0000\n",
            "Epoch [70/100], Loss: 0.0001\n",
            "Epoch [70/100], Loss: 0.0001\n",
            "Epoch [70/100], Loss: 0.0001\n",
            "Epoch [70/100], Loss: 0.0001\n",
            "Epoch [70/100], Loss: 0.0001\n",
            "Epoch [70/100], Loss: 0.0001\n",
            "Epoch [70/100], Loss: 0.0002\n",
            "Epoch [70/100], Loss: 0.0002\n",
            "Epoch [70/100], Loss: 0.0002\n",
            "Epoch [70/100], Loss: 0.0002\n",
            "Epoch [70/100], Loss: 0.0002\n",
            "Epoch [70/100], Loss: 0.0002\n",
            "Epoch [70/100], Loss: 0.0002\n",
            "Epoch [70/100], Loss: 0.0002\n",
            "Epoch [70/100], Loss: 0.0003\n",
            "Epoch [70/100], Loss: 0.0003\n",
            "Epoch [70/100], Loss: 0.0003\n",
            "Epoch [70/100], Loss: 0.0003\n",
            "Epoch [70/100], Loss: 0.0003\n",
            "Epoch [70/100], Loss: 0.0003\n",
            "Epoch [70/100], Loss: 0.0003\n",
            "Epoch [70/100], Loss: 0.0003\n",
            "Epoch [70/100], Loss: 0.0003\n",
            "Epoch [70/100], Loss: 0.0003\n",
            "Epoch [70/100], Loss: 0.0004\n",
            "Epoch [70/100], Loss: 0.0004\n",
            "Epoch [70/100], Loss: 0.0004\n",
            "Epoch [70/100], Loss: 0.0004\n",
            "Epoch [70/100], Loss: 0.0004\n",
            "Epoch [70/100], Loss: 0.0005\n",
            "Epoch [70/100], Loss: 0.0005\n",
            "Epoch [70/100], Loss: 0.0005\n",
            "Epoch [70/100], Loss: 0.0005\n",
            "Epoch [70/100], Loss: 0.0005\n",
            "Epoch [70/100], Loss: 0.0005\n",
            "Epoch [70/100], Loss: 0.0005\n",
            "Epoch [70/100], Loss: 0.0005\n",
            "Epoch [70/100], Loss: 0.0006\n",
            "Epoch [70/100], Loss: 0.0006\n",
            "Epoch [70/100], Loss: 0.0006\n",
            "Epoch [70/100], Loss: 0.0006\n",
            "Epoch [70/100], Loss: 0.0006\n",
            "Epoch [70/100], Loss: 0.0006\n",
            "Epoch [70/100], Loss: 0.0006\n",
            "Epoch [70/100], Loss: 0.0007\n",
            "Epoch [80/100], Loss: 0.0000\n",
            "Epoch [80/100], Loss: 0.0000\n",
            "Epoch [80/100], Loss: 0.0000\n",
            "Epoch [80/100], Loss: 0.0000\n",
            "Epoch [80/100], Loss: 0.0000\n",
            "Epoch [80/100], Loss: 0.0001\n",
            "Epoch [80/100], Loss: 0.0001\n",
            "Epoch [80/100], Loss: 0.0001\n",
            "Epoch [80/100], Loss: 0.0001\n",
            "Epoch [80/100], Loss: 0.0001\n",
            "Epoch [80/100], Loss: 0.0001\n",
            "Epoch [80/100], Loss: 0.0001\n",
            "Epoch [80/100], Loss: 0.0001\n",
            "Epoch [80/100], Loss: 0.0001\n",
            "Epoch [80/100], Loss: 0.0001\n",
            "Epoch [80/100], Loss: 0.0002\n",
            "Epoch [80/100], Loss: 0.0002\n",
            "Epoch [80/100], Loss: 0.0002\n",
            "Epoch [80/100], Loss: 0.0002\n",
            "Epoch [80/100], Loss: 0.0002\n",
            "Epoch [80/100], Loss: 0.0002\n",
            "Epoch [80/100], Loss: 0.0002\n",
            "Epoch [80/100], Loss: 0.0002\n",
            "Epoch [80/100], Loss: 0.0002\n",
            "Epoch [80/100], Loss: 0.0002\n",
            "Epoch [80/100], Loss: 0.0002\n",
            "Epoch [80/100], Loss: 0.0002\n",
            "Epoch [80/100], Loss: 0.0002\n",
            "Epoch [80/100], Loss: 0.0003\n",
            "Epoch [80/100], Loss: 0.0003\n",
            "Epoch [80/100], Loss: 0.0003\n",
            "Epoch [80/100], Loss: 0.0003\n",
            "Epoch [80/100], Loss: 0.0003\n",
            "Epoch [80/100], Loss: 0.0003\n",
            "Epoch [80/100], Loss: 0.0003\n",
            "Epoch [80/100], Loss: 0.0004\n",
            "Epoch [80/100], Loss: 0.0004\n",
            "Epoch [80/100], Loss: 0.0004\n",
            "Epoch [80/100], Loss: 0.0004\n",
            "Epoch [80/100], Loss: 0.0004\n",
            "Epoch [80/100], Loss: 0.0004\n",
            "Epoch [80/100], Loss: 0.0004\n",
            "Epoch [80/100], Loss: 0.0004\n",
            "Epoch [80/100], Loss: 0.0004\n",
            "Epoch [80/100], Loss: 0.0004\n",
            "Epoch [80/100], Loss: 0.0004\n",
            "Epoch [80/100], Loss: 0.0004\n",
            "Epoch [80/100], Loss: 0.0005\n",
            "Epoch [80/100], Loss: 0.0005\n",
            "Epoch [90/100], Loss: 0.0000\n",
            "Epoch [90/100], Loss: 0.0000\n",
            "Epoch [90/100], Loss: 0.0000\n",
            "Epoch [90/100], Loss: 0.0000\n",
            "Epoch [90/100], Loss: 0.0000\n",
            "Epoch [90/100], Loss: 0.0000\n",
            "Epoch [90/100], Loss: 0.0001\n",
            "Epoch [90/100], Loss: 0.0001\n",
            "Epoch [90/100], Loss: 0.0001\n",
            "Epoch [90/100], Loss: 0.0001\n",
            "Epoch [90/100], Loss: 0.0001\n",
            "Epoch [90/100], Loss: 0.0001\n",
            "Epoch [90/100], Loss: 0.0001\n",
            "Epoch [90/100], Loss: 0.0001\n",
            "Epoch [90/100], Loss: 0.0001\n",
            "Epoch [90/100], Loss: 0.0001\n",
            "Epoch [90/100], Loss: 0.0001\n",
            "Epoch [90/100], Loss: 0.0001\n",
            "Epoch [90/100], Loss: 0.0001\n",
            "Epoch [90/100], Loss: 0.0001\n",
            "Epoch [90/100], Loss: 0.0001\n",
            "Epoch [90/100], Loss: 0.0002\n",
            "Epoch [90/100], Loss: 0.0002\n",
            "Epoch [90/100], Loss: 0.0002\n",
            "Epoch [90/100], Loss: 0.0002\n",
            "Epoch [90/100], Loss: 0.0002\n",
            "Epoch [90/100], Loss: 0.0002\n",
            "Epoch [90/100], Loss: 0.0002\n",
            "Epoch [90/100], Loss: 0.0002\n",
            "Epoch [90/100], Loss: 0.0002\n",
            "Epoch [90/100], Loss: 0.0002\n",
            "Epoch [90/100], Loss: 0.0002\n",
            "Epoch [90/100], Loss: 0.0002\n",
            "Epoch [90/100], Loss: 0.0002\n",
            "Epoch [90/100], Loss: 0.0002\n",
            "Epoch [90/100], Loss: 0.0003\n",
            "Epoch [90/100], Loss: 0.0003\n",
            "Epoch [90/100], Loss: 0.0003\n",
            "Epoch [90/100], Loss: 0.0003\n",
            "Epoch [90/100], Loss: 0.0003\n",
            "Epoch [90/100], Loss: 0.0003\n",
            "Epoch [90/100], Loss: 0.0003\n",
            "Epoch [90/100], Loss: 0.0003\n",
            "Epoch [90/100], Loss: 0.0003\n",
            "Epoch [90/100], Loss: 0.0003\n",
            "Epoch [90/100], Loss: 0.0003\n",
            "Epoch [90/100], Loss: 0.0003\n",
            "Epoch [90/100], Loss: 0.0003\n",
            "Epoch [90/100], Loss: 0.0004\n",
            "Epoch [100/100], Loss: 0.0000\n",
            "Epoch [100/100], Loss: 0.0000\n",
            "Epoch [100/100], Loss: 0.0000\n",
            "Epoch [100/100], Loss: 0.0000\n",
            "Epoch [100/100], Loss: 0.0000\n",
            "Epoch [100/100], Loss: 0.0000\n",
            "Epoch [100/100], Loss: 0.0000\n",
            "Epoch [100/100], Loss: 0.0000\n",
            "Epoch [100/100], Loss: 0.0001\n",
            "Epoch [100/100], Loss: 0.0001\n",
            "Epoch [100/100], Loss: 0.0001\n",
            "Epoch [100/100], Loss: 0.0001\n",
            "Epoch [100/100], Loss: 0.0001\n",
            "Epoch [100/100], Loss: 0.0001\n",
            "Epoch [100/100], Loss: 0.0001\n",
            "Epoch [100/100], Loss: 0.0001\n",
            "Epoch [100/100], Loss: 0.0001\n",
            "Epoch [100/100], Loss: 0.0001\n",
            "Epoch [100/100], Loss: 0.0001\n",
            "Epoch [100/100], Loss: 0.0001\n",
            "Epoch [100/100], Loss: 0.0001\n",
            "Epoch [100/100], Loss: 0.0001\n",
            "Epoch [100/100], Loss: 0.0001\n",
            "Epoch [100/100], Loss: 0.0001\n",
            "Epoch [100/100], Loss: 0.0001\n",
            "Epoch [100/100], Loss: 0.0001\n",
            "Epoch [100/100], Loss: 0.0001\n",
            "Epoch [100/100], Loss: 0.0001\n",
            "Epoch [100/100], Loss: 0.0001\n",
            "Epoch [100/100], Loss: 0.0002\n",
            "Epoch [100/100], Loss: 0.0002\n",
            "Epoch [100/100], Loss: 0.0002\n",
            "Epoch [100/100], Loss: 0.0002\n",
            "Epoch [100/100], Loss: 0.0002\n",
            "Epoch [100/100], Loss: 0.0002\n",
            "Epoch [100/100], Loss: 0.0002\n",
            "Epoch [100/100], Loss: 0.0002\n",
            "Epoch [100/100], Loss: 0.0002\n",
            "Epoch [100/100], Loss: 0.0002\n",
            "Epoch [100/100], Loss: 0.0002\n",
            "Epoch [100/100], Loss: 0.0002\n",
            "Epoch [100/100], Loss: 0.0002\n",
            "Epoch [100/100], Loss: 0.0002\n",
            "Epoch [100/100], Loss: 0.0002\n",
            "Epoch [100/100], Loss: 0.0002\n",
            "Epoch [100/100], Loss: 0.0002\n",
            "Epoch [100/100], Loss: 0.0002\n",
            "Epoch [100/100], Loss: 0.0003\n",
            "Epoch [100/100], Loss: 0.0003\n",
            "\n",
            "Word embeddings:\n",
            "deep: [ 1.3423235e+00 -1.6635489e+00  5.3646600e-01  1.3441798e-01\n",
            "  5.8005172e-01 -8.8931817e-01 -1.2043906e+00 -8.7312007e-01\n",
            "  4.3609205e-01  2.8637969e+00 -1.8959600e+00 -6.0942400e-01\n",
            " -1.8332468e+00  6.3445795e-01  2.0037314e-01 -1.2093500e+00\n",
            " -1.8195824e-01 -1.5730157e+00 -3.3391303e-01 -8.6618954e-01\n",
            " -9.8306745e-01  2.9520690e-01 -9.7716373e-01  5.2517664e-01\n",
            "  7.8061759e-01 -3.9278874e-01 -4.4984993e-01 -8.8663232e-01\n",
            " -1.2577633e+00  1.7154276e+00 -3.4585401e-01  4.8501971e-01\n",
            "  6.1871046e-01  1.9889870e+00 -1.2716299e+00 -9.0922213e-01\n",
            "  1.3801308e+00 -9.1471724e-02 -2.0419068e+00 -7.7250510e-02\n",
            "  4.0693030e-01 -9.4640964e-01  5.8541145e-02 -1.3435839e-01\n",
            "  1.0893608e+00  2.4196583e-01 -5.6501174e-01 -1.3717782e+00\n",
            " -1.3320101e+00  1.6115509e-02 -3.0457169e-01 -1.0566860e+00\n",
            "  3.2094005e-01  1.6335626e+00  1.0661012e-01  2.2538546e-02\n",
            "  6.4644814e-01  3.9380679e-01 -2.3495425e-01  4.7806641e-01\n",
            "  2.6221395e+00  4.4673780e-01  2.1371830e+00 -3.8986942e-01\n",
            " -3.2924646e-01 -4.8854294e-01  1.6110978e+00 -1.2838587e+00\n",
            " -6.3956714e-01  6.7272669e-01  9.1743857e-01  5.5603951e-01\n",
            " -9.9963528e-01  5.6133145e-01 -1.3249118e+00 -4.9047628e-01\n",
            " -6.0997522e-01 -6.9017076e-01 -6.2951750e-01 -2.4087927e-01\n",
            " -8.9429194e-01 -2.0854900e+00 -4.1197538e-01  9.3403739e-01\n",
            "  5.5684328e-01  6.5473604e-01 -8.3409238e-01 -1.6668615e+00\n",
            "  1.5381185e+00  1.1241339e+00 -1.4346555e+00  8.7984312e-01\n",
            "  1.6619115e+00  1.7267632e+00  1.0092824e+00  1.1214318e+00\n",
            "  1.9062141e-01 -3.8336083e-02  1.7225593e-01  7.0982349e-01\n",
            "  2.1853539e-01 -3.3429071e-01 -8.6617106e-01 -2.5098005e-01\n",
            "  1.7366718e+00 -1.9316987e+00 -9.1037834e-01  4.5361432e-01\n",
            " -1.0421102e+00 -3.2354558e+00 -1.7593424e+00  1.5082923e-01\n",
            "  9.5707983e-01  3.0627129e+00 -2.1190351e-01  8.8259298e-01\n",
            " -9.1440278e-01  7.4756992e-01  8.2616735e-01  1.1236484e+00\n",
            "  7.5301111e-01  2.6006106e-01  3.2380110e-01  3.6135498e-01\n",
            " -2.7835846e-01  4.4001499e-01  7.4737132e-01  7.2155201e-01\n",
            "  4.4764882e-01  1.2279739e+00  4.1343746e-01 -1.3416208e+00\n",
            "  1.8850256e+00 -1.8431094e-01  2.1500177e+00 -7.3268592e-01\n",
            " -1.5052283e+00 -7.8039306e-01  8.5989863e-01 -2.1354134e+00\n",
            "  1.5192552e+00 -1.2617983e-01 -6.3077039e-01  8.9242351e-01\n",
            "  2.1067643e+00  2.9995582e-01 -2.8124356e+00  5.3590190e-01\n",
            " -4.4888136e-01 -2.8597149e-01  8.4044027e-01 -7.8398323e-01\n",
            "  3.3575273e-01  1.6974774e-01  1.2699273e-01  6.1213571e-01\n",
            "  9.0776145e-01  1.2591513e+00  1.8934529e+00  1.6806829e-01\n",
            " -2.0420752e+00  8.8439995e-01 -7.6177269e-02  7.3890626e-01\n",
            " -5.7005209e-01 -1.3197244e+00 -1.5885746e+00  1.8818369e-03\n",
            " -1.8079129e+00 -1.1625034e+00 -3.4523484e-01  6.2580615e-01\n",
            " -2.4134365e-01  1.2021214e+00 -7.3855418e-01  7.8885424e-01\n",
            "  6.5004492e-01  2.3181970e+00 -6.7011511e-01  2.2237062e-02\n",
            "  7.1645206e-01  1.5583168e+00  2.5746610e+00 -3.9158660e-01\n",
            "  4.7005782e-01  5.8255935e-01  9.3002576e-01  7.1662444e-01\n",
            " -1.0751835e+00 -1.1744523e+00 -6.9425607e-01  1.3107783e+00\n",
            "  1.9637581e+00  1.1098621e+00  2.1717303e-01  9.5646769e-01\n",
            "  1.1384602e+00 -1.9343712e+00 -9.7663963e-01 -3.9637727e-01\n",
            "  7.4350202e-01 -1.7789813e-02  8.7859869e-01  1.4099795e+00\n",
            " -1.0322231e+00 -1.8515648e-01 -1.1882948e+00  4.9275836e-01\n",
            "  4.7488925e-01 -9.3912852e-01 -7.8542769e-01 -1.0889887e+00\n",
            " -1.7218567e-01 -6.9630343e-01  3.1505796e-01 -4.4438934e-01\n",
            " -1.0897197e+00  4.5582452e-01  5.1147050e-01 -6.6672122e-01\n",
            "  5.6387144e-01 -6.7738372e-01 -1.4456521e+00 -2.3623291e-01\n",
            "  1.1100490e+00  5.0761515e-01 -9.4933516e-01  1.2381195e+00\n",
            "  3.8143858e-01  4.6194407e-01 -2.0989172e+00 -1.3249296e+00\n",
            "  1.2646431e+00  5.7137448e-01 -3.3446807e-01  1.2710676e+00\n",
            "  1.2063743e+00  8.8360870e-01  8.1841731e-01  8.4204549e-01\n",
            " -1.3560604e+00  1.1208359e+00  1.3169919e+00 -3.9656532e-01\n",
            "  7.7376819e-01 -1.0926485e+00  6.3057966e-03  1.7159562e-01\n",
            " -4.6623805e-01 -9.0612358e-01 -3.9200288e-01 -4.6882305e-01\n",
            "  8.2163000e-01  2.6864583e+00 -1.1575481e+00 -6.9867718e-01\n",
            " -2.9237700e-01 -1.3905851e+00  2.7211329e-02 -1.2834181e-01\n",
            " -4.3848714e-01 -1.6092318e+00  8.2917535e-01  6.1843753e-01\n",
            "  1.0258918e+00 -1.3526669e+00  1.5865659e+00 -2.3166761e-03\n",
            " -3.1572962e-01  9.3539381e-01 -3.6903780e-02 -1.2527717e+00\n",
            "  6.2545073e-01 -3.5939065e-01 -1.4900972e+00 -1.9662325e-01\n",
            "  1.2965504e+00  4.4162169e-01 -6.0677487e-01  3.5281595e-02\n",
            " -1.0948530e+00 -5.0222993e-01  6.9750226e-01  6.8898183e-01\n",
            " -2.0692857e-01  5.2309901e-01 -2.5655949e-01 -5.6491476e-01\n",
            "  2.9111046e-01 -1.0217363e+00  8.7742567e-02  5.8306962e-01\n",
            "  9.3137074e-01  4.9069038e-01 -4.8059890e-01 -1.3509746e-01\n",
            "  5.2000093e-01  7.1134603e-01  1.3783640e-01  5.2220756e-01]\n",
            "learning: [ 0.23497394  0.58058447  0.31612706 -0.8923164  -0.09886593  0.55351627\n",
            " -1.3602326  -1.1412836  -0.3281764  -0.9586871  -0.13204111  0.69686496\n",
            "  0.59554344 -0.48908672  2.4357848   0.92320114  0.39372674 -1.0285285\n",
            " -0.6801618  -2.2276857  -1.4555528  -0.8149445   0.44066045  0.17737702\n",
            " -0.5790071  -0.9837314  -0.26040563 -1.0903035   0.6193987  -1.3047003\n",
            "  1.2717806   0.47342974  0.6311131   0.8468414   0.16162473  0.07333925\n",
            "  1.6898624   0.99463236  0.5890365   1.1121914  -0.19540481 -0.38245907\n",
            "  0.00741721 -0.59088534  0.30381078  0.09859977 -1.2817619  -0.14459763\n",
            " -0.16620949 -1.2401812   0.8179145   1.4124838   0.48229104 -0.538014\n",
            " -0.8660414   0.92618215 -1.0215435  -1.4925089  -0.651498    0.39470497\n",
            " -1.1145294   0.97531635 -0.3424322   1.4104742  -0.90342337  0.11438059\n",
            "  0.24515036  1.2514597  -0.95962    -0.5839726   0.9789813   0.36143196\n",
            " -1.180736   -0.5012523  -0.7217158  -0.69697696 -3.543946    0.26891762\n",
            " -1.8420825  -1.1305265  -0.50802517 -0.56305456 -0.09457508 -0.90529996\n",
            "  0.14752336  0.49355486 -0.36418405  0.5238392  -1.4985867   0.7555522\n",
            " -0.25653768 -1.796557   -0.57394946 -0.300994    0.799087    0.8262043\n",
            " -0.2020245  -1.3950232   1.0009874  -0.48680794 -0.3505631  -0.42995766\n",
            "  1.9938353  -1.7406063  -0.57929957 -0.1717727  -0.6615601  -1.7424381\n",
            "  0.7213029  -0.22507103 -1.3488545  -0.92900836  0.92875814 -0.375177\n",
            "  0.24284558 -1.4695811  -0.10445926 -1.3055615   0.949358   -1.100344\n",
            "  0.8679517  -1.0609674  -0.81434995  1.0961899  -1.0932465   0.2731632\n",
            "  1.7356465  -1.1797245   0.080964   -0.15363328  0.24758668 -1.127802\n",
            " -0.46881455 -0.15676957  0.7566757  -1.40662    -0.86792856 -0.34623572\n",
            "  2.0751464   0.3389273  -1.0144721   0.51558703  1.0704517  -0.37881094\n",
            " -0.28200305  0.1688813   0.5096384  -0.78013647 -2.9066167  -0.310721\n",
            " -0.4641761   0.19225942  1.3172411   0.53721094 -0.34194988  0.43634847\n",
            " -0.10416413  0.20087801 -0.13581565  0.69151795  0.70065576  0.28542995\n",
            " -0.98674643 -1.1373225   1.7133465  -0.12983496 -2.2078004   0.39781204\n",
            "  0.9327298   0.23075306 -1.046485    0.62074816 -0.34912604  0.8048513\n",
            "  0.36755273  0.6914445  -0.6695576  -1.1366466   0.5054972  -0.9410576\n",
            "  0.35647362 -0.652553   -0.8763846   0.9824395  -2.3576517  -2.6093829\n",
            " -1.9043248   0.65834284  2.0102522  -0.7357006   0.8542675   0.77724934\n",
            "  0.56449306 -0.35627836  1.001826   -0.82448417 -0.6485453  -1.6845438\n",
            " -1.0176314  -0.7233107  -1.7111747   0.25623742 -0.47661215  0.9426249\n",
            "  0.15672718 -2.142559   -0.3965511  -0.25790343  0.58339167  1.304579\n",
            "  0.77408004  0.42798185  0.2832018   0.3551592  -0.7175662   1.2482967\n",
            "  0.7616627  -1.460213   -0.6196145   0.2674274   1.3643968  -2.1360588\n",
            "  0.73694044  0.2604232   0.7915532  -0.09994083  1.3745102  -0.07795013\n",
            "  1.4951395  -0.5755203  -0.48359314 -0.6523849  -0.6884442  -1.4464563\n",
            "  1.607057    0.61328673  1.8134274  -0.723325    1.2249235  -0.43818566\n",
            "  0.14110775 -0.23367915 -0.9025485  -0.07120381  0.38045374 -1.3678776\n",
            " -0.989276   -0.1202986  -1.109317    0.33315414 -0.16378945 -0.84890115\n",
            " -0.12222029  0.38258687  0.55084616 -0.19897075 -1.2162366  -0.7480435\n",
            "  0.4797806   2.7399638  -2.8160596   0.5559071   0.38230455 -0.32140255\n",
            "  0.6121488  -1.7697756  -1.3389391   0.4904055  -0.64790666  1.8355731\n",
            " -1.127484    0.32809544 -0.3703549   0.506787    0.1409147   0.3779797\n",
            "  0.8741548  -1.8923178  -0.24633399  0.55929506  0.4141086   1.869162\n",
            " -0.15869954  1.5060974   2.8071303  -1.4252425   0.64231867 -0.5219576\n",
            " -1.5690633  -1.2080113   0.27134287 -0.9105931  -0.7087162   3.0824542\n",
            "  1.1414227   1.5320585  -1.3874253  -1.1754762  -0.33330995 -1.0405663 ]\n",
            "intelligence: [ 1.7008758  -0.4503503   0.2355793   1.1409363   0.800974   -0.04402224\n",
            " -0.6057837   1.1639633   0.8688526   1.3147568  -0.30975777  0.38410053\n",
            "  0.14645521  1.5793197  -1.0675286  -0.3379296  -0.48560667 -0.49144536\n",
            "  0.39811295 -1.8046551  -1.1492087  -2.2195992   0.703144    1.6861063\n",
            "  0.03307936  0.9376926  -1.9749029  -0.899209    0.7259096   0.3511542\n",
            " -0.65118897 -0.5749121  -1.2044276  -0.50233597  0.10730998 -1.3664384\n",
            "  1.2034113   0.46558002 -3.1930707   0.9315541   0.3841186   1.0685378\n",
            " -0.22334561 -0.879695    0.8045741  -1.455257    0.01390066 -2.4888349\n",
            " -1.4516399   1.0591927  -0.5328886  -0.02185477 -1.2054063   1.644878\n",
            "  0.7011853  -0.38379204  0.2636316   0.3066856  -1.1109508   1.0678045\n",
            "  0.441715   -0.4366363   1.3885796   0.35026893  0.98744315  1.2001379\n",
            "  0.8120238   2.6150975  -0.4600392   0.27637476  1.6823174  -0.5222029\n",
            "  0.32190916 -0.2981662   0.8247828  -2.7340412  -1.8616421   0.11882157\n",
            " -2.1545863   0.44442797  1.5008385   0.37013933 -0.95732254  0.09472698\n",
            "  1.282538   -0.49393523 -0.02676304  3.3601835   0.2620785   0.65872586\n",
            " -0.4761328   1.6324779  -1.3387561   1.4255477   2.4557397   1.3181896\n",
            "  1.870883   -0.29281518  0.84948695  0.33969143 -0.7241366   0.47844407\n",
            "  0.28270337  0.03167895  1.4791602  -0.10358367 -0.55697036 -1.7013754\n",
            " -0.29714927  1.3895985  -0.33867553 -0.8181919   1.502634   -1.1011679\n",
            "  0.26961017  0.778705    2.2484624   0.74084854 -0.45319998 -0.39371097\n",
            "  1.1493397  -0.6540119   1.6204468  -0.8738289   2.801269   -2.3894613\n",
            " -0.05820201  0.69549924 -0.07420762  1.1327161   1.5162895  -0.04188767\n",
            "  0.9652415  -0.18125235 -0.7349877   0.90399444 -0.6299266   0.81560004\n",
            " -0.01464408  0.00556458 -0.8347827   0.22132875  1.3130034  -0.9515643\n",
            "  1.4123026   0.55425435 -1.212903   -1.1494116  -1.0354859  -0.09322888\n",
            "  1.530044   -1.41171    -1.1932929  -0.5560176   0.01642353 -0.01638625\n",
            "  0.06716421 -0.6534987  -1.0089262  -0.53134364  0.1469945  -1.5401335\n",
            "  0.459967   -1.6862453  -1.0182955   1.9829645   0.6042661  -0.849061\n",
            " -2.3393505   1.0447005  -1.7758425   1.3703077  -0.5134719  -0.12931241\n",
            " -1.1724932   0.2643291   0.5811653  -0.07067284  1.4699466   0.12644416\n",
            " -0.13025188  1.5268131  -0.5809271   0.3394266  -0.09844043  0.4170801\n",
            "  0.28160053  1.4179286   1.2536975   0.2169149   1.886993    1.612087\n",
            " -0.01091411  0.13052274 -2.7505574   1.8229882   0.9678832   0.5093532\n",
            " -1.7388744   0.86047083 -0.699194    0.18871821  0.99944526 -2.1409123\n",
            "  1.1931305   0.6401121   0.05446924 -0.378995    1.4420906   0.6149744\n",
            " -0.42139184 -2.3203094   0.60517824  0.2730079   0.41521722  0.36480287\n",
            " -0.7812488  -1.4232873   0.3512373  -0.752118   -0.01677406 -0.6638958\n",
            "  0.19970295 -1.1059885  -1.1245214   0.5343639  -2.1266203   0.87750375\n",
            " -1.3588809   1.9507389   0.56259984  1.8441358   0.4733372  -0.04254376\n",
            " -0.14967598 -1.347448    0.3551721  -0.08415017 -0.5016499  -1.3473525\n",
            "  1.1926268  -1.6520413  -0.5035629   1.0943375  -0.17207107 -0.0423224\n",
            "  0.41918963  0.14745241 -0.7218392   1.6059989   0.29314318 -0.39101067\n",
            "  1.7613485  -0.11837661 -1.7924457  -0.64101595  0.30908385  0.7389745\n",
            " -0.508938   -0.56834716 -0.42208624  1.5897692   1.1693047  -0.26928884\n",
            " -2.9497643  -0.08284117 -0.7465631   2.24033     0.44054943  1.2132704\n",
            "  0.34566605  0.16616808  1.460146   -1.0056117   0.6567358  -0.45341998\n",
            " -1.4530392  -1.9595912   0.70561653 -0.60928786  0.43130884  0.3064113\n",
            "  0.14420594 -0.19301859  0.4009118  -0.3851875  -0.13393563  0.98935086\n",
            " -0.8014665   0.83046275  0.36168185  0.93739724 -0.7289244   0.9199218\n",
            "  0.43069646 -1.270597   -1.6912705  -0.9802689  -0.10007995 -0.23384632]\n",
            "network: [-1.1206226   0.46929976 -1.2475082  -0.4968033  -0.81215364  0.986324\n",
            " -0.89545155 -0.49714336 -0.8108639  -0.28703278 -1.0430897   0.38558236\n",
            " -0.38009346  0.6577991  -0.57209116 -0.5124606  -1.1808584   0.8209754\n",
            "  0.5388342   0.36665422  0.35352615 -0.8063917  -0.87446547  1.6248353\n",
            " -0.0806017   0.03627003  1.4182904   0.0943373  -0.35752738 -1.5286359\n",
            "  0.14290072  0.59779644  0.9021042   1.9239448   1.2024854   0.25987428\n",
            "  0.40910777  0.41919965 -0.00503314  1.7657218  -0.49631053 -0.9097308\n",
            " -0.2798299  -1.1026864  -2.0326424  -1.2997792  -0.0596413   0.6169917\n",
            " -0.5653299  -0.13858587 -0.57027864 -0.00796393 -0.03660374 -1.0081657\n",
            " -1.0371897   2.6007214   0.50280863 -1.0976678   2.294191    0.30446175\n",
            "  0.39377758  0.2915537   0.82436234 -1.0620675  -1.9359453   0.5003206\n",
            "  1.0862509  -1.1990756   0.5722062   0.41971946 -1.8580048   0.8741552\n",
            " -0.74731714  0.9396884   0.9168732  -0.374982   -0.31577465  0.69951075\n",
            "  0.69048643  1.2785275  -0.29485     0.6861151  -0.25641778  1.4946313\n",
            " -0.37433788  1.5751268  -0.12581801 -0.24936435 -0.30283865  0.7476671\n",
            " -0.59002084 -1.0066515   1.0152053  -0.35993308  1.7766483   0.14970684\n",
            "  2.192142    1.1255614   0.5685001   1.8481747  -0.7610472  -1.2762871\n",
            " -0.9061149  -0.9471791   0.52847755 -0.71912974 -0.74762464  0.7965363\n",
            "  1.5744615   0.4992149   0.5208625   1.6654015   0.2764946  -1.2409607\n",
            " -1.2829058   0.92293334  0.2792265   0.9671421   1.1450374  -0.2661592\n",
            "  0.4835037  -0.73895204 -0.4066459  -0.70401853  1.6084485   1.0525317\n",
            "  0.20320821 -0.2947971   1.0127094  -1.2391763  -0.36698148  0.9518979\n",
            " -0.04410775 -0.38815716  1.6931221   1.0805794   1.2174757   2.1171784\n",
            " -0.21023537  1.7899673   1.9819084  -0.27944666  0.76290464 -0.7145382\n",
            "  1.9418652  -1.0738676   0.7295225  -1.3115475   0.7993577  -0.6990147\n",
            " -0.7173117  -1.5185556  -0.04750445  1.480288   -1.7294652   1.3610003\n",
            " -0.15146978  0.8719234  -0.5496832  -0.47097534 -1.6953915   0.86437553\n",
            "  0.32487965 -0.635689   -0.7858946  -0.83757377 -0.11262869 -0.67260003\n",
            " -0.13339676  0.20789109 -0.41153008  0.8266552   0.9325274   0.19653969\n",
            " -0.73195     0.52807665  1.1733284  -0.38597798  0.07874259  0.38175565\n",
            " -1.5881846  -0.17645283 -0.6878827   0.7080699  -0.55411875 -0.5835981\n",
            " -1.0408401  -0.35562286 -0.50272304  1.5883068  -0.31893906 -1.3427792\n",
            "  0.0466677   0.35959074 -0.25196782  0.20154984  0.09569424  0.8786357\n",
            "  2.07518     0.5370294  -1.0200137   0.4856418  -0.6969587   0.5102851\n",
            " -0.99113774  1.5583943  -0.41771835  0.35292044 -0.38791507  1.0980865\n",
            "  0.94722885 -0.43464506  1.3745263   0.75868016  0.9782632  -1.7942606\n",
            " -1.151543    0.17737459  1.3407533  -1.1963626   0.26145145 -1.570719\n",
            "  1.5798938  -1.10155    -0.17034604  0.12037086 -0.5411737  -1.286857\n",
            " -0.32446003 -0.60009336  0.2784581  -0.8280759   0.3966491   0.5881504\n",
            "  1.2820139  -0.19519305  0.5085836  -0.98276126  1.8272828   0.409466\n",
            "  0.5866264  -0.6855875   1.3367965  -1.7300042   1.4287658   0.14250085\n",
            "  2.4390507   0.2973348   0.25724283  0.60353935  1.8300854   0.95508\n",
            "  2.7282798   0.17890452  0.64743066  1.0866114  -0.55811816  0.55307\n",
            " -0.28713062 -0.6052359   0.84454644 -0.7813904   1.3022902  -0.6287979\n",
            "  1.2482715  -1.4320291   0.490542   -0.47282106 -0.07720867  0.05318451\n",
            " -0.43062368 -0.2893979  -0.29758662  0.44758633  0.8951796   0.6497135\n",
            " -1.9934219  -0.05157532 -0.5258242  -1.946106   -0.6026017   1.5946316\n",
            "  0.89122355  1.4285022  -1.0674429  -1.167263   -1.1580374   0.11457033\n",
            "  0.89783657  0.06252659 -0.21707885  0.3619433   0.7844435  -2.0602355\n",
            "  2.2965612  -0.6577597  -0.55528885  0.27078968 -0.5444098  -0.0783911 ]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "class WordEmbeddingModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(WordEmbeddingModel, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "      embeds = self.embeddings(inputs)\n",
        "      output = self.linear(embeds)\n",
        "      return output\n",
        "\n",
        "vector_size = 300\n",
        "model = WordEmbeddingModel(vocab_size, vector_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "training_data = []\n",
        "for sentence in tokenized_corpus:\n",
        "    for i, target_word in enumerate(sentence):\n",
        "        target_idx = word2idx[target_word]\n",
        "        context_idx = word2idx[target_word]\n",
        "        training_data.append((context_idx, target_idx))\n",
        "\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for context_idx, target_idx in training_data:\n",
        "        context_tensor = torch.LongTensor([context_idx])\n",
        "        target_tensor = torch.LongTensor([target_idx])\n",
        "\n",
        "        outputs = model(context_tensor)\n",
        "        loss = criterion(outputs, target_tensor)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "          print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(training_data):.4f}')\n",
        "\n",
        "word_embeddings = model.embeddings.weight.data\n",
        "\n",
        "print(\"\\nWord embeddings:\")\n",
        "for word in [\"deep\", \"learning\", \"intelligence\", \"network\"]:\n",
        "    if word in word2idx:\n",
        "        idx = word2idx[word]\n",
        "        print(f\"{word}: {word_embeddings[idx].numpy()}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
